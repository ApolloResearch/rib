exp_name: edge_ablations_train_set_tinystories_ground_truth_dist_sammples=2500_10%gram
ablation_type: edge
rib_results_path:
schedule:
  schedule_type: bisect
  score_target: 3.20
dataset:
  dataset_type: huggingface
  name: roneneldan/TinyStories # or skeskinen/TinyStories-GPT4, but not clear if part of training
  tokenizer_name: EleutherAI/gpt-neo-125M
  return_set: train
  return_set_frac: null
  return_set_n_samples: 50 # avg ~235 toks / story
  return_set_portion: first #TODO last
  n_ctx: 200 # needs to be <= 511 for the model to behave reasonably
ablation_node_layers:
  - ln1.0
  - ln1_out.0
  - attn_in.0
  - ln2.0
  - ln2_out.0
  - mlp_in.0
  - ln1.1
  - ln1_out.1
  - attn_in.1
  - ln2.1
  - ln2_out.1
  - mlp_in.1
  - ln1.2
  - ln1_out.2
  - attn_in.2
  - ln2.2
  - ln2_out.2
  - mlp_in.2
  - ln1.3
  - ln1_out.3
  - attn_in.3
  - ln2.3
  - ln2_out.3
  - mlp_in.3
  - ln1.4
  - ln1_out.4
  - attn_in.4
  - ln2.4
  - ln2_out.4
  - mlp_in.4
  - ln1.5
  - ln1_out.5
  - attn_in.5
  - ln2.5
  - ln2_out.5
  - mlp_in.5
  - ln1.6
  - ln1_out.6
  - attn_in.6
  - ln2.6
  - ln2_out.6
  - mlp_in.6
  - ln1.7 # resid
  - ln1_out.7 # var, resid
  - attn_in.7 # ln_resid, resid
  - ln2.7 # resid, attn_out + resid
  - ln2_out.7 # var, resid, attn_out + resid
  - mlp_in.7 # ln_resid, attn_out + resid
  - ln_final
  - ln_final_out
  - unembed # resid #this would be ==ln1.8 if there was one
batch_size: 1000
dtype: float32
eval_type: ce_loss
seed: 0