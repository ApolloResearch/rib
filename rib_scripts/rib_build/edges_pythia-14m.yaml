exp_name: pythia-14m
seed: 0
tlens_pretrained: pythia-14m
tlens_model_path: null
interaction_matrices_path: rib_scripts/rib_build/out/pythia-14m_rib_Cs.pt
dataset:
  dataset_type: huggingface
  name: NeelNanda/pile-10k
  tokenizer_name: EleutherAI/pythia-14m
  return_set: train  # pile-10k only has train, so we take the first 90% for building and last 10% for ablations
  return_set_frac: 0.1
  n_documents: null
  n_samples: null
  return_set_portion: first
  n_ctx: 50
node_layers:
  - mlp_out.0
  - ln2.3
  - mlp_out.3
  - ln1.5
  - mlp_out.5
batch_size: 6  # Based on 24GB a5000 GPU
gram_batch_size: 10  # Based on 24GB a5000 GPU
edge_batch_size: 8  # Based on 24GB a5000 GPU, although increasing/decreasing doesn't seem to affect overall runtime
truncation_threshold: 1e-15
rotate_final_node_layer: true
n_intervals: 50
dtype: float64
calculate_edges: true
eval_type: null