{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from pydantic import BaseModel, field_validator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from rib.data_accumulator import collect_gram_matrices, collect_interaction_edges\n",
    "from rib.hook_manager import HookedModel\n",
    "from rib.interaction_algos import InteractionRotation, calculate_interaction_rotations\n",
    "from rib.log import logger\n",
    "from rib.models import MLP\n",
    "from rib.plotting import plot_interaction_graph\n",
    "from rib.types import TORCH_DTYPES\n",
    "from rib.utils import REPO_ROOT, check_outfile_overwrite, load_config, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IdentityWideHidden(MLP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=4,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        super(IdentityWideHidden, self).__init__(\n",
    "            hidden_sizes = [2*input_size],\n",
    "            input_size=input_size,\n",
    "            output_size=input_size,\n",
    "            dtype=dtype,\n",
    "            fold_bias=False,\n",
    "            activation_fn='relu'\n",
    "        )\n",
    "        W_embed = torch.zeros(4*input_size-2)\n",
    "        W_embed[2*input_size-2:2*input_size] = torch.tensor([-1,1])\n",
    "        W_embed = W_embed.as_strided((input_size, 2*input_size), (2,1)).flip(dims = (1,))\n",
    "        self.layers[0].W = nn.Parameter(W_embed)\n",
    "        self.layers[1].W = nn.Parameter(W_embed.T)\n",
    "        for i in range(2):\n",
    "            self.layers[i].b = nn.Parameter(torch.zeros_like(self.layers[i].b))\n",
    "        self.fold_bias()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "input_size = 2\n",
    "dtype=torch.float32\n",
    "stds = [1, 1]\n",
    "mean = torch.zeros((dataset_size, input_size), dtype=dtype)\n",
    "stds = torch.tensor(stds).broadcast_to((dataset_size, input_size))\n",
    "print(mean.shape,stds.shape)\n",
    "torch.normal(mean,stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IdentityWideHidden(input_size=4)\n",
    "a = torch.randn(4).unsqueeze(0)\n",
    "print(a)\n",
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "W_embed = torch.zeros(4*input_size-2)\n",
    "W_embed[2*input_size-2:2*input_size] = torch.tensor([-1,1])\n",
    "print(W_embed)\n",
    "W_embed = W_embed.as_strided((input_size, 2*input_size), (2,1)).flip(dims = (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_embed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
