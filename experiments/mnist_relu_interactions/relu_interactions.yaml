exp_name: relu_interactions
mlp_path: /mnt/ssd-apollo/models/MNIST/checkpoints/lr-0.001_bs-64_2023-08-31_10-42-52/model_epoch_12.pt
batch_size: 256
seed: 0
truncation_threshold: 1e-3
n_intervals: 0
rotate_output: false
dtype: float32
module_names:
  - layers.0.activation
<<<<<<< HEAD
  - layers.1.activation
=======
  - layers.1.activation
  - layers.2.activation
  - layers.3.activation
mlp_config:
  hidden_sizes: [50, 50, 50]
  input_size: 784
  output_size: 10
  activation_fn: "relu"
  bias: true
  fold_bias: true
>>>>>>> a520a013c7ebc61674b3837aa26c2999d4b29100
