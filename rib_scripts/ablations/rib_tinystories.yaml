exp_name: rib_tinystories_stochastic_5
ablation_type: edge
rib_results_path: /mnt/ssd-interp/stefan/rib/rib_scripts/rib_build/out/tinystories_stochastic_5_rib_graph.pt
# rib_results_path: /mnt/ssd-interp/stefan/rib/rib_scripts/rib_build/out/tinystories_rib_graph.pt
schedule:
  schedule_type: bisect
  score_target: 3.20
dataset:
  dataset_type: huggingface
  name: roneneldan/TinyStories # or skeskinen/TinyStories-GPT4, but not clear if part of training
  tokenizer_name: EleutherAI/gpt-neo-125M
  return_set: train
  return_set_frac: null
  return_set_n_samples: 50 # avg ~235 toks / story
  return_set_portion: first
  n_ctx: 10 # needs to be <= 511 for the model to behave reasonably
ablation_node_layers:  # Rotate the input to these modules into the interaction basis
  - ln1.1
  - ln1.3
  - ln1.5
  - ln1.7
# - unembed
batch_size: 1000
dtype: float32
eval_type: ce_loss
seed: 0
