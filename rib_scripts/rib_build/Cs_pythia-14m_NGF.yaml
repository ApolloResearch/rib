exp_name: pythia-14m-manylayers-NGF
seed: 0
tlens_pretrained: pythia-14m
tlens_model_path: null
interaction_matrices_path: null
dataset:
  dataset_type: huggingface
  name: NeelNanda/pile-10k
  tokenizer_name: EleutherAI/pythia-14m
  return_set: train  # pile-10k only has train, so we take the first 90% for building and last 10% for ablations
  return_set_frac: null
  n_samples: 10
  n_documents: 10
  return_set_portion: first
  n_ctx: 100
node_layers:
  - ln1.3
  - ln1_out.3
  - attn_in.3
  - ln2.3
  - ln2_out.3
  - mlp_in.3
  - ln1.4
  - ln1_out.4
  - attn_in.4
  - ln2.4
  - ln2_out.4
  - mlp_in.4
  - ln1.5
  - ln1_out.5
  - attn_in.5
  - ln2.5
  - ln2_out.5
  - mlp_in.5
batch_size: 25  #  A100 can handle 24
gram_batch_size: 40  #  A100 can handle 80
truncation_threshold: 1e-10
rotate_final_node_layer: false
n_intervals: 0
integration_method: gradient
dtype: float64
calculate_edges: false
basis_formula: jacobian
edge_formula: squared
center: true
n_stochastic_sources_basis_hidden: 50
n_stochastic_sources_basis_pos: 10
naive_gradient_flow: true
# using approx 8GB VRAM
#/home/user/rib/rib_scripts/rib_build/out/pythia-14m-NGF_rib_Cs.pt
#/home/user/rib/rib_scripts/rib_build/out/pythia-14m_rib_Cs.pt