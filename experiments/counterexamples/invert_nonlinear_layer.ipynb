{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Callable, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from rib.data_accumulator import collect_gram_matrices, collect_interaction_edges\n",
    "from rib.hook_manager import HookedModel\n",
    "from rib.interaction_algos import InteractionRotation, calculate_interaction_rotations\n",
    "from rib.log import logger\n",
    "from rib.models import MLP\n",
    "from rib.plotting import plot_interaction_graph\n",
    "from rib.types import TORCH_DTYPES\n",
    "from rib.utils import REPO_ROOT, check_outfile_overwrite, set_seed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertNonlinear(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, nonlinearity):\n",
    "        super(InvertNonlinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_inputs, bias=False)\n",
    "\n",
    "        # Set the nonlinearity\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        # Freeze the parameters of the first layer\n",
    "        for param in self.fc2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nonlinearity(self.fc1(x))\n",
    "        x = self.nonlinearity(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityWideHidden(MLP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=4,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        super(IdentityWideHidden, self).__init__(\n",
    "            hidden_sizes=[2 * input_size],\n",
    "            input_size=input_size,\n",
    "            output_size=input_size,\n",
    "            dtype=dtype,\n",
    "            fold_bias=False,\n",
    "            activation_fn=\"gelu\",\n",
    "        )\n",
    "        W_embed = torch.zeros(4 * input_size - 2, dtype=dtype)\n",
    "        W_embed[2 * input_size - 2 : 2 * input_size] = torch.tensor([-1, 1])\n",
    "        W_embed = W_embed.as_strided((input_size, 2 * input_size), (2, 1)).flip(dims=(1,))\n",
    "        # random_mix = torch.randn(input_size, input_size)\n",
    "        self.layers[0].W = nn.Parameter(W_embed)\n",
    "        self.layers[1].W = nn.Parameter(W_embed.T)\n",
    "        # self.layers[0].W = nn.Parameter(random_mix @ W_embed)\n",
    "        # self.layers[1].W = nn.Parameter(W_embed.T @ torch.linalg.inv(random_mix))\n",
    "        for i in range(2):\n",
    "            self.layers[i].b = nn.Parameter(torch.zeros_like(self.layers[i].b, dtype=dtype))\n",
    "        # self.fold_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mixers(\n",
    "    n_inputs: int,\n",
    "    hidden_sizes: Union[int, list[int]],\n",
    "    fs: list[Callable],\n",
    "    activation_fn: str = \"relu\",\n",
    "    datasize: int = 2**24,\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 2**10,\n",
    "    lr: float = 3e-6,\n",
    "    data_variance: int = 1,\n",
    "    print_number: int = 20,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if isinstance(hidden_sizes, int):\n",
    "        hidden_sizes = [hidden_sizes]\n",
    "    input_data = data_variance * torch.rand((datasize, n_inputs)).to(device) + 0.1\n",
    "    mixer_target = torch.stack([f(input_data[:, 0], input_data[:, 1]) for f in fs], dim=1)\n",
    "    mixer_target = mixer_target.to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(input_data, mixer_target)\n",
    "    input_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    mixer = MLP(\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        input_size=n_inputs,\n",
    "        output_size=len(fs),\n",
    "        activation_fn=activation_fn,\n",
    "    ).to(device)\n",
    "    # new variable unmixer is exactly the same architecture as mixer]\n",
    "    unmixer = MLP(\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        input_size=len(fs),\n",
    "        output_size=n_inputs,\n",
    "        activation_fn=activation_fn,\n",
    "    ).to(device)\n",
    "    mixer_optimizer = torch.optim.Adam(mixer.parameters(), lr=lr)\n",
    "    unmixer_optimizer = torch.optim.Adam(unmixer.parameters(), lr=lr)\n",
    "    # first train the mixer\n",
    "    total = datasize // batch_size\n",
    "    all_steps = 0\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for step, (input, target) in enumerate(tqdm(input_loader, total=total)):\n",
    "            all_steps += 1\n",
    "            mixer_optimizer.zero_grad()\n",
    "            output = mixer(input)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            mixer_optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            if step % (total // print_number) == 0:\n",
    "                plt.semilogy(losses)\n",
    "                plt.show()\n",
    "                test_data = data_variance * torch.randn(1, n_inputs).to(device)\n",
    "                output = mixer(test_data)\n",
    "                target = torch.stack([f(test_data[:, 0], test_data[:, 1]) for f in fs], dim=1)\n",
    "                print(\"data:\", test_data, \"\\noutput:\", output, \"\\ntarget\", target)\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    # then train the unmixer\n",
    "    all_steps = 0\n",
    "    losses2 = []\n",
    "    for epoch in range(epochs):\n",
    "        for step, (input, target) in enumerate(tqdm(input_loader, total=total)):\n",
    "            all_steps += 1\n",
    "            unmixer_optimizer.zero_grad()\n",
    "            output = unmixer(mixer(input))\n",
    "            loss = F.mse_loss(output, input)\n",
    "            loss.backward()\n",
    "            unmixer_optimizer.step()\n",
    "            losses2.append(loss.item())\n",
    "            if step % (total // print_number) == 0:\n",
    "                plt.semilogy(losses2)\n",
    "                plt.show()\n",
    "                test_input = data_variance * torch.randn(1, n_inputs).to(device)\n",
    "                target = torch.stack([f(test_input[:, 0], test_input[:, 1]) for f in fs], dim=1)\n",
    "                output = unmixer(mixer(test_input))\n",
    "                print(\"input:\", test_input, \"\\noutput:\", output, \"\\ntarget\", target)\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    return mixer, unmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "def f2(x, y):\n",
    "    return x / y\n",
    "\n",
    "\n",
    "fs = [f1, f2]\n",
    "mixer, unmixer = train_mixers(2, [1000, 1000, 1000, 1000], fs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.randn(1, 2)\n",
    "print(random)\n",
    "print(mixer(random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "mixer = MLP(hidden_sizes=[100], input_size=n_inputs, output_size=n_inputs, activation_fn=\"relu\")\n",
    "unmixer = MLP(hidden_sizes=[100], input_size=n_inputs, output_size=n_inputs, activation_fn=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0002\n",
    "steps = 2**25\n",
    "n_inputs = 5\n",
    "batch_size = 2**8\n",
    "n_hidden = 20\n",
    "print_num = 20\n",
    "\n",
    "# Data (random data for demonstration)\n",
    "# In practice, this should be your specific data\n",
    "input_data = 10 * torch.randn(steps, n_inputs).to(device)\n",
    "# input_data = torch.eye(n_inputs).to(device)\n",
    "\n",
    "# Dataset and DataLoader for mini-batch training\n",
    "dataset = torch.utils.data.TensorDataset(input_data, input_data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Network\n",
    "network = InvertNonlinear(n_inputs, n_hidden, F.gelu).to(device)\n",
    "\n",
    "# Optimizer (update parameters of the second layer only)\n",
    "optimizer = optim.Adam(network.fc1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for step, (inputs, targets) in tqdm(enumerate(dataloader), total=steps // batch_size):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = network(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % ((steps // batch_size) // print_num) == 0:\n",
    "        print(f\"step {step+1}/{steps}, Loss: {loss.item()}\")\n",
    "        print(network(torch.eye(n_inputs).to(device)))\n",
    "    if loss.item() < 1e-6:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
