exp_name: tinystories_ngf_sammples=1k
seed: 0
tlens_pretrained: tiny-stories-1M
tlens_model_path: null
dataset:
  dataset_type: huggingface
  name: roneneldan/TinyStories # or skeskinen/TinyStories-GPT4, but not clear if part of training
  tokenizer_name: EleutherAI/gpt-neo-125M
  return_set: train
  return_set_frac: null
  n_documents: 10000  # avg ~235 toks / document
  n_samples: 1000
  # 2k: 97.1% during mlp_in.7 after >1 batch
  # 20: 95.1% during mlp_in.7 after >1 batch
  return_set_portion: first
  n_ctx: 100 # needs to be <= 511 for the model to behave reasonably
node_layers:
  # - ln1.0
  # - ln1_out.0
  # - attn_in.0
  # - ln2.0
  # - ln2_out.0
  # - mlp_in.0
  # - ln1.1
  # - ln1_out.1
  # - attn_in.1
  # - ln2.1
  # - ln2_out.1
  # - mlp_in.1
  # - ln1.2
  # - ln1_out.2
  # - attn_in.2
  # - ln2.2
  # - ln2_out.2
  # - mlp_in.2
  # - ln1.3
  # - ln1_out.3
  # - attn_in.3
  # - ln2.3
  # - ln2_out.3
  # - mlp_in.3
  # - ln1.4
  # - ln1_out.4
  # - attn_in.4
  # - ln2.4
  # - ln2_out.4
  # - mlp_in.4
  # - ln1.5
  # - ln1_out.5
  # - attn_in.5
  # - ln2.5
  # - ln2_out.5
  # - mlp_in.5
  # - ln1.6
  # - ln1_out.6
  # - attn_in.6
  # - ln2.6
  # - ln2_out.6
  # - mlp_in.6
  - ln1.7 # resid
  - ln1_out.7 # var, resid
  - attn_in.7 # ln_resid, resid
  - ln2.7 # resid, attn_out + resid
  - ln2_out.7 # var, resid, attn_out + resid
  - mlp_in.7 # ln_resid, attn_out + resid
  - ln_final
  - ln_final_out
  - unembed # resid #this would be ==ln1.8 if there was one
rotate_final_node_layer: true
# Good barch sizes for 48GB a6000 with n_ctx=256
gram_batch_size: 500
batch_size: 600
edge_batch_size: 400
truncation_threshold: 1e-15
dtype: float64
calculate_edges: true
eval_type: null
# edges 7.63 min, Cs 1.49 min
n_intervals: 0
integration_method: gradient
basis_formula: jacobian
edge_formula: squared
center: true
n_stochastic_sources_basis_pos: 20
n_stochastic_sources_basis_hidden: 40
n_stochastic_sources_edges: 6 #over pos, up to 511
# d_hidden = 64
# in_grads: Float[Tensor, "n_phis batch in_pos in_hidden"]
# n_stochastic_sources_basis_pos * n_stochastic_sources_basis_hidden * batch_size * n_ctx * n_hidden
# = 64 * 256 * 3 * 256 * 64 = 6.7e8 is 90% VRAM usage
naive_gradient_flow: true